[Data]
pretrained_embeddings_file = giga.100.txt
# BERT
bert_path = my-bert
bert_config = %(bert_path)s/bert_config.json
bert_vocab_file = my-bert/vocab.txt
# m-BERT
mbert_path = MultilingualModle/my-mbert
mbert_config = %(mbert_path)s/config.json
mbert_vocab_file = %(mbert_path)s/vocab.txt
# XLM-R
xlmr_path = MultilingualModle/my-xlmr
xlmr_config = %(xlmr_path)s/config.json
xlmr_tokenizer_file = MultilingualModle/my-xlmr

data_dir = cross_lingual_dataset
train_file = %(data_dir)s/en/en_ewt-ud-train.conllu
train_file_weight = %(data_dir)s/en/en_ewt-ud-train-weight.conllu
dev_file = %(data_dir)s/en/en_ewt-ud-dev.conllu

target_lan = de
source_tt_train_file = %(data_dir)s/%(target_lan)s/source+tt.conllu
tt_vocab_file = %(data_dir)s/%(target_lan)s/en-%(target_lan)s-tt-0.1_parallel.tgt.conllu
tt_weight_align_train_file = %(data_dir)s/%(target_lan)s/weight-en-%(target_lan)s-tt.tgt.conllu
tt_weight_cos_train_file = %(data_dir)s/%(target_lan)s/weight-cos-en-%(target_lan)s_tt_head_rel-0.1_parallel.tgt.conllu
tt_weight_cos_source_train_file = %(data_dir)s/%(target_lan)s/weight-cos-en-%(target_lan)s_source_head_rel-0.1.tgt.conllu

relabel_file = %(tt_weight_align_train_file)s.relabel_iteration_head_rel-0.1-0.6-0.2-earlystop_parallel_new_clean
relabel_cos_0_file = %(tt_weight_align_train_file)s.relabel_cos_0_head_rel
relabel_cos_cos_file = %(tt_weight_align_train_file)s.relabel_cos_cos_head_rel
# tt_weight_align_cos_train_file = %(data_dir)s/%(target_lan)s/weight-align+cos-en-%(target_lan)s_clean.tgt.conllu
# tt_weight_train_file = MLP_RESULT/weight_train/1536-200-200-50-1.conllu

target_file = %(data_dir)s/%(target_lan)s/de_gsd-ud-train.conllu
test_file = %(data_dir)s/%(target_lan)s/de_gsd-ud-test.conllu

nosiy_file = %(data_dir)s/noisy.conll
min_occur_count = 2
nosiy_sigma = 3.5

[Save]
# BERT
save_dir = ModelResult/bert
config_file = %(save_dir)s/config.cfg
save_model_path = %(save_dir)s/model
save_vocab_path = %(save_dir)s/vocab
load_dir = ModelResult/bert
load_model_path = %(load_dir)s/model
load_vocab_path = %(load_dir)s/vocab

[Network]
lstm_layers = 3
tune_start_layer = 12
word_dims = 100
tag_dims = 100
dropout_emb = 0.33
lstm_hiddens = 400
dropout_lstm_input = 0.33
dropout_lstm_hidden = 0.33
mlp_arc_size = 500
mlp_rel_size = 100
dropout_mlp = 0.33
output_hidden_states = True
output_attentions = False

[Optimizer]
learning_rate = 2e-3
decay = .75
decay_steps = 1000
beta_1 = .9
beta_2 = .9
epsilon = 1e-12
clip = 5.0

[Run]
num_buckets_train = 40
num_buckets_valid = 10
num_buckets_test = 10
train_iters = 30
max_train_len = 200
train_batch_size = 50
test_batch_size = 50
validate_every = 165
save_after = 5000
update_every = 4

