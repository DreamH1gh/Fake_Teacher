from transformers import BertTokenizer

class BertTokenHelper(object):
    def __init__(self, bert_vocab_file):
        self.tokenizer = BertTokenizer.from_pretrained(bert_vocab_file)
        print("Load bert vocabulary finished")
        self.key_words = ("[UNK]", "[SEP]","[PAD]", "[CLS]", "[MASK]")

    def bert_ids(self, text, length):
        text = text.replace('##', '@@')
        text = text.replace('``', "''")
        text = text.replace("’", "'")
        text = text.replace("‘", "'")
        text = text.replace("`", "'")
        text = text.replace("ı", "i")
        text = text.replace('i̇', 'i')
        text = text.replace(' ͡', ' ')
        text = text.replace("….", "…")
        text = text.replace('\u200b\u200b', "")
        text = text.replace('\xad', '')
        text = text.replace('“…', '“')
        text = text.replace('\xa0', "")
        text = text.replace('–', '-')
        text = text.replace('у́', 'y')
        text = text.replace('о́', 'o')
        text = text.replace('שׂ', 'ש')
        text = text.replace('רָ', 'ר')
        text = text.replace('מִ', 'מ')
        text = text.replace('נָ', 'נ')
        text = text.replace('רִ', 'ר')
        text = text.replace('קְ', 'ק')
        text = text.replace('שָׁ', 'ש')
        text = text.replace('פְּ', 'פ')
        text = text.replace('סֵ', 'ס')
        text = text.replace('וֹ', 'ו')
        text = text.replace('לִ', 'ל')
        text = text.replace('בָּ', 'ב')
        text = text.replace('מְ', 'מ')
        text = text.replace('וּ', 'ו')
        text = text.replace('קַ', 'ק')
        text = text.replace('שׁ', 'ש')
        text = text.replace('תִ', 'ת')
        text = text.replace('מֵ', 'מ')
        text = text.replace('אָ', 'א')
        text = text.replace('חַ', 'ח')
        text = text.replace('דָ', 'ד')
        text = text.replace('חָ', 'ח')
        text = text.replace('הָ', 'ה')
        text = text.replace('נֵ', 'נ')
        text = text.replace('שִׁ', 'ש')
        text = text.replace('מָ', 'מ')
        text = text.replace('פִ', 'פ')
        text = text.replace('טְ', 'ט')
        text = text.replace('רֶ', 'ר')
        text = text.replace('כּ', 'כ')
        text = text.replace('שְ', 'ש')
        text = text.replace('קָ', 'ק')
        text = text.replace('תַ', 'ת')
        text = text.replace('כֵ', 'כ')
        text = text.replace('רַ', 'ר')
        text = text.replace('לְ', 'ל')
        text = text.replace('עַ', 'ט')
        text = text.replace('בֵּ', 'ב')
        text = text.replace('מַ', 'מ')
        text = text.replace('יָ', 'י')
        text = text.replace('הִ', 'ה')
        text = text.replace('לָ', 'ל')
        text = text.replace('הַ', 'ה')
        text = text.replace('אִ', 'א')
        text = text.replace('עֲ', 'ע')
        text = text.replace('סִ', 'ס')
        text = text.replace('עֶ', 'ע')
        text = text.replace('פָּ', 'פ')
        text = text.replace('אֵ', 'א')
        text = text.replace('לֹא', 'לא')
        text = text.replace('לֶ', 'ל')
        text = text.replace('יִ', 'י')
        text = text.replace('פַּ', 'פ')
        text = text.replace('אֲ', 'א')
        text = text.replace('הֵ', 'ה')
        text = text.replace('זָ', 'ז')
        text = text.replace('מֶ', 'מ')
        text = text.replace('אֱ', 'א')
        text = text.replace('בֶּ', 'ב')
        text = text.replace('נִ', 'נ')
        text = text.replace('סַ', 'ס')
        text = text.replace('זֶ', 'ז')
        text = text.replace('חְ', 'ח')
        text = text.replace('תָ', 'ת')
        text = text.replace('لٰ', 'ل')
        text = text.replace('هٰ', 'ه')
        text = text.replace('ذٰ', 'ذ')
        text = text.replace('اً', 'ا')
        text = text.replace('نً', 'ن')
        text = text.replace('رّ', 'ر')
        text = text.replace('وّ', 'و')
        text = text.replace('جً', 'ج')
        text = text.replace('يّ', 'ي')
        text = text.replace('مُ', 'م')
        text = text.replace('مّ', 'م')
        text = text.replace('أ', 'ا')
        text = text.replace('يُ', 'ي')
        text = text.replace('زّ', 'ز')
        text = text.replace('سّ', 'س')
        text = text.replace('ـّ', 'ـ')
        text = text.replace('بّ', 'ب')
        text = text.replace('فّ', 'ف')
        text = text.replace('دّ', 'د')
        text = text.replace('وُ', 'و')
        text = text.replace('تّ', 'ت')
        text = text.replace('قُ', 'ق')
        text = text.replace('ءً', 'ء')
        text = text.replace('يْ', 'ي')
        text = text.replace('لّ', 'ل')
        text = text.replace('لَ', 'ل')
        text = text.replace('لاّ', 'لا')
        text = text.replace('كّ', 'ك')
        text = text.replace('ةٌ', 'ة')
        text = text.replace('اُ', 'ا')
        text = text.replace('اِ', 'ا')
        text = text.replace('تُ', 'ت')
        text = text.replace('سُ', 'س')
        text = text.replace('فِ', 'ف')
        text = text.replace('قً', 'ق')
        text = text.replace('دً', 'د')
        text = text.replace('مَ', 'م')
        text = text.replace('جّ', 'ج')
        text = text.replace('طُ', 'ط')
        text = text.replace('قَ', 'ق')
        text = text.replace('سَ', 'س')
        text = text.replace('ةً', 'ة')
        text = text.replace('جَ', 'ج')
        text = text.replace('رَ', 'ر')
        text = text.replace('رُ', 'ر')
        text = text.replace('قّ', 'ق')
        text = text.replace('بِ', 'ب')
        text = text.replace('نّ', 'ن')
        text = text.replace('اَ', 'ا')
        text = text.replace('خُ', 'خ')
        text = text.replace('حّ', 'ح')
        text = text.replace('ذّ', 'ذ')
        text = text.replace('يً', 'ي')
        text = text.replace('خّ', 'خ')
        text = text.replace('ىً', 'ى')
        text = text.replace('لٍ', 'ل')
        text = text.replace('رً', 'ر')
        text = text.replace('سً', 'س')
        text = text.replace('ضً', 'ض')
        text = text.replace('مً', 'م')
        text = text.replace('بً', 'ب')
        text = text.replace('فٍ', 'ف')
        text = text.replace('طً', 'ط')
        text = text.replace('عً', 'ع')
        text = text.replace('لًا', '')
        text = text.replace('ئً', 'ي')
        text = text.replace('فً', 'ق')
        text = text.replace('زً', 'ر')
        text = text.replace('كً', 'ك')
        text = text.replace('حً', 'ح')
        text = text.replace('صً', 'ص')
        text = text.replace('ذً', 'ذ')
        text = text.replace('تً', 'ت')
        text = text.replace('غً', 'غ')
        text = text.replace('اً', 'ا')
        text = text.replace('نُ', 'ن')
        text = text.replace('ثً', 'ث')
        text = text.replace('شً', 'ش')
        text = text.replace('هً', 'ه')
        text = text.replace('زُ', 'ز')
        text = text.replace('ظً', 'ظ')
        text = text.replace('خً', 'خ')
        text = text.replace('وً', 'و')
        text = text.replace('وٍ', 'و')
        text = text.replace('كَ', 'ك')
        text = text.replace('مِ', 'م')
        text = text.replace('عِ', 'ع')
        text = text.replace('بُ', 'ب')
        text = text.replace('نٍ', 'ن')
        text = text.replace('دٍ', 'د')
        text = text.replace('صُ', 'ص')
        text = text.replace('ضّ', 'ض')
        text = text.replace('ضٍ', 'ض')
        text = text.replace('هُ', 'ه')
        text = text.replace('قِ', 'ق')
        text = text.replace('عُ', 'ع')
        text = text.replace('تِّ', 'ت')
        text = text.replace('о́', 'o')
        text = text.replace('о́', 'o')


        outputs = self.tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')
        bert_indice = outputs["input_ids"].squeeze(0)
        segments_id = outputs["token_type_ids"].squeeze(0)

        list_bert_indice = [idx.item() for idx in bert_indice]
        list_segments_id = [idx.item() for idx in segments_id]

        bert_tokens = self.tokenizer.convert_ids_to_tokens(list_bert_indice)
        org_tokens = text.split()
        tokens = self.tokenizer.convert_tokens_to_string(bert_tokens)

        list_piece_id = []
        org_id, last_id = -1, -1
        last_word = ''
        able_continue = False
        for idx, bpe_u in enumerate(bert_tokens):
            if bpe_u in self.key_words:
                last_id += 1
                if bpe_u == '[UNK]': org_id += 1
                last_word = bpe_u
                list_piece_id.append([idx])
                able_continue = False
            elif bpe_u.startswith("##") or (able_continue and len(last_word) < len(org_tokens[org_id])):
            # elif bpe_u.startswith("##"):
                list_piece_id[last_id].append(idx)
                if bpe_u.startswith("##"): last_word = last_word + bpe_u[2:]
                else: last_word = last_word + bpe_u
                able_continue = True
            else:
                last_id += 1
                org_id += 1
                last_word = bpe_u
                able_continue = True
                list_piece_id.append([idx])
        if org_id != length-1:
            print("An error occurs in bert tokenizer: token len not aligns")

        return list_bert_indice, list_segments_id, list_piece_id
